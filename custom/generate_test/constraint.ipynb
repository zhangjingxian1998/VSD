{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangjx/anaconda3/envs/vlt5/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import PhrasalConstraint, DisjunctiveConstraint\n",
    "# checkpoint = \"/home/nlp_weight/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"people want to eat fruit like\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' are'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(389)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people want to eat fruit like bananas watermelon world people subway car is a good idea but it's not the best way of getting around.\n",
      "I'm going with this one because\n"
     ]
    }
   ],
   "source": [
    "constraints = [\n",
    "    PhrasalConstraint(\n",
    "        tokenizer(\"watermelon world people subway kill\", add_prefix_space=True, add_special_tokens=False).input_ids\n",
    "    )\n",
    "]\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "result = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "for i in result:\n",
    "    print(i)\n",
    "# without constraints\n",
    "# people want to eat fruit like apples and oranges,\" he said. \"But they don't know what it is.\"\n",
    "# with constraints\n",
    "# people want to eat fruit like oranges and bananas,\" he said. \"They don't have the money for that.\"\n",
    "# , which is a lot of sugar in it —watermelon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people want to eat fruit like bananas banana world people subway car is watermelon and you can't get it in the supermarket.\n",
      "I'm not saying that I don' think there\n",
      "people want to eat fruit like bananas banana world people subway car is watermelon and you can't get it in the supermarket.\n",
      "I'm not saying that I don' think apple\n"
     ]
    }
   ],
   "source": [
    "constraints = [\n",
    "    DisjunctiveConstraint(\n",
    "        [\n",
    "        tokenizer(\"watermelon world people subway kill\", add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "        tokenizer(\"banana world people subway kill\", add_prefix_space=True,add_special_tokens=False).input_ids,\n",
    "         tokenizer(\"apple world people subway kill\", add_prefix_space=True,add_special_tokens=False).input_ids,\n",
    "         tokenizer(\"pear world people subway kill\", add_prefix_space=True,add_special_tokens=False).input_ids,\n",
    "         ]\n",
    "    ),\n",
    "]\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=2,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "result = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "for i in result:\n",
    "    print(i)\n",
    "\n",
    "# without constraints\n",
    "# people want to eat fruit like apples and oranges,\" he said. \"But they don't know what it is.\"\n",
    "\n",
    "# with constraints\n",
    "# people want to eat fruit like oranges and bananas, but they don't have the money. They're not going hungry.\"\n",
    "# - The New York Times\n",
    "\n",
    "# \"I'mpear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1660, 45690, 661, 995, 1266, 835]]]\n",
      "people want to eat fruit like oranges and bananas, watermelon is a good choice,\" said Dr. David Siegelman of the University's School for Health Policy Research in New York\n",
      "people want to eat fruit like oranges and bananas, watermelon is a good choice,\" said Dr. David Siegelman of the University's School for Health Policy watermelon people world\n",
      "people want to eat fruit like oranges and bananas, watermelon is a good choice,\" said Dr. David Siegelman of the University's School for Health Policy Research watermelon people\n",
      "people want to eat fruit like oranges and bananas, watermelon is a good choice,\" said Dr. David Siegelman of the University's School for Health Policy Research in watermelon\n",
      "people want to eat fruit like oranges and bananas, watermelon is a good choice,\" said Dr. David Siegelman of the University's School for Health Policy Research in New water\n"
     ]
    }
   ],
   "source": [
    "force_word = \"watermelon people world best way\" # PhrasalConstraint\n",
    "# force_flexible = [\"banana \", \"apple\", \"pear \"] # DisjunctiveConstraint\n",
    "force_words_ids = [\n",
    "    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "    # tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "]\n",
    "print(force_words_ids)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=5,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    max_new_tokens=30\n",
    ")\n",
    "result = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "for i in result:\n",
    "    print(i)\n",
    "# without force_word and force_flexible\n",
    "# people want to eat fruit like apples and oranges,\" he said. \"But they don't know what it is.\"\n",
    "\n",
    "# with force_word\n",
    "# people want to eat fruit like oranges and bananas,\" he said. \"They don't have the money for that.\"\n",
    "# , which is a lot of sugar in it's watermelon\n",
    "\n",
    "# with force_flexible\n",
    "# people want to eat fruit like oranges and bananas,\" he said. \"But they don't have the money for that, so it's not a big deal.\"\n",
    "#  (Photo apple\n",
    "\n",
    "# with force_word and force_flexible\n",
    "# people want to eat fruit like oranges and pearls, but they don't have the money. They're not going hungry.\"\n",
    "\n",
    "\n",
    "# The problem is that many of these watermelon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people want to eat fruit like apples and oranges,\"Anchor said. \"It's not just about the price, but also how much you can get out of it.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forced_decoder_ids = [[10,2025]] # 2025 An\n",
    "forced_decoder_ids = [[10,47811]] # 47811 suggests\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    max_new_tokens=30,\n",
    "    forced_decoder_ids=forced_decoder_ids\n",
    ")\n",
    "result = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "for i in result:\n",
    "    print(i)\n",
    "\n",
    "# without forced_decoder_ids\n",
    "# people want to eat fruit like apples and oranges,\" he said. \"But they don't know what it is.\"\n",
    "\n",
    "# with forced_decoder_ids An\n",
    "# people want to eat fruit like apples and oranges,\"Anchor said. \"It's not just about the price, but also how much you can get out of it.\"\n",
    "\n",
    "# with forced_decoder_ids suggests\n",
    "# people want to eat fruit like apples and oranges,suggested by a group of scientists at the National Institute on Environmental Health Sciences (NIEHS) in New York City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore is the only country in Asia that does not have a state-run health insurance system. The government of Singapore has been trying to make it easier for\n"
     ]
    }
   ],
   "source": [
    "prefix = 'Singapore'\n",
    "prefix_ids = tokenizer.encode(prefix, return_tensors='pt')\n",
    "def prefix_allowed_tokens_fn(batch_id, sent):\n",
    "    # 如果是生成的第一个令牌，只允许与前缀匹配的令牌\n",
    "    if sent[-1] == tokenizer.encode(prefix)[-1]:\n",
    "        allow_list = [tokenizer.encode('and',add_prefix_space=True)]\n",
    "        return allow_list\n",
    "    elif sent[-1] == tokenizer.encode('and',add_prefix_space=True)[-1]:\n",
    "        allow_list = [tokenizer.encode('China',add_prefix_space=True)]\n",
    "        return allow_list\n",
    "    else:\n",
    "        # 其他情况不限制\n",
    "        allow_list = list(range(tokenizer.vocab_size))\n",
    "        return allow_list\n",
    "outputs = model.generate(\n",
    "    prefix_ids,\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    max_new_tokens=30,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "result = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "for i in result:\n",
    "    print(i)\n",
    "\n",
    "# without prefix_allowed_tokens_fn\n",
    "# Singapore is the only country in Asia that does not have a state-run health insurance system. The government of Singapore has been trying to make it easier for\n",
    "\n",
    "# with prefix_allowed_tokens_fn\n",
    "# SingaporeandChina are the only two countries in Asia to have a single-payer health care system. The other is Japan, which has one of its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7050, 45690]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"watermelon\",add_special_tokens=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers             4.30.2\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlt5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae1457347cd33692b46213e19d6845ccfc4f5742d4a3151710a79af9631c4975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
